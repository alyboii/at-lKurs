{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8537fbf-96c0-4d42-9997-aeb60c19bcef",
   "metadata": {},
   "source": [
    "# üöÄ Regression Models Encyclopedia: Which One Should I Choose?\n",
    "\n",
    "In Data Science, there is no \"Single Best Model.\" You must choose the right weapon for the battle. Here is your complete guide.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Basic Models (No Punishment)\n",
    "The goal of these models is simply to minimize the **Error** (Residuals). They do not care if the math gets complex.\n",
    "\n",
    "### üîπ 1. Simple Linear Regression\n",
    "* **The Logic:** \"One Input, One Output.\" Draws a straight line.\n",
    "* **The Formula:** $$y = b_0 + b_1x$$\n",
    "* **Real World Example:** Predicting a child's **Height** based ONLY on their **Age**.\n",
    "* **When to use:** When you have a single feature and the relationship looks straight.\n",
    "\n",
    "### üîπ 2. Multiple Linear Regression\n",
    "* **The Logic:** \"Many Inputs, One Output.\"\n",
    "* **The Formula:** $$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$$\n",
    "* **Critical Rule:** You MUST use **StandardScaler**! Otherwise, big numbers (Salary: 50,000) will dominate small numbers (Age: 30).\n",
    "* **Real World Example:** Predicting **House Price** based on *Size, Location, Floor, and Age*.\n",
    "\n",
    "### üîπ 3. Polynomial Regression\n",
    "* **The Logic:** \"The world is curved, not flat.\" We use powers ($x^2, x^3$) to capture curves.\n",
    "* **The Math:** It is actually Multiple Regression, but with engineered features:\n",
    "  $$y = b_0 + b_1x + b_2x^2 + b_3x^3$$\n",
    "* **The Risk:** If you choose a high Degree (e.g., 10), the model will **Overfit** (memorize the noise).\n",
    "* **Real World Example:** **Temperature Prediction**. It is cold in the morning, hot at noon, and cold at night. A straight line fails here; you need a \"U\" curve.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Regularization Models (The Punishers)\n",
    "If your model is **Overfitting** (memorizing), we add a \"Penalty\" to the math.\n",
    "\n",
    "> **üí° The Philosophy:** \"You can make a model, but you are NOT allowed to use huge coefficients (weights). Keep it simple.\"\n",
    "\n",
    "### üî∏ 4. Ridge Regression (L2 Regularization) - \"The Volume Control\"\n",
    "* **How it works:** It shrinks the coefficients (weights) towards zero, but they never reach exactly zero.\n",
    "* **The Penalty Math:** Adds $$\\lambda \\sum (\\text{slope})^2$$ to the Error equation.\n",
    "    * Because we **square** the slope, large numbers (like 10,000) get a **huge** penalty. The model is forced to pick small numbers.\n",
    "* **Best For:**\n",
    "    * When you want to keep ALL your features.\n",
    "    * When you have **Multicollinearity** (duplicate features like \"Age in Years\" and \"Age in Months\").\n",
    "* **Analogy:** In a loud meeting, you don't kick anyone out, but you turn down everyone's volume.\n",
    "\n",
    "### üî∏ 5. Lasso Regression (L1 Regularization) - \"The Mute Button\"\n",
    "* **How it works:** It shrinks coefficients all the way to **Zero (0)**. It effectively deletes useless features.\n",
    "* **The Penalty Math:** Adds $$\\lambda \\sum |\\text{slope}|$$ to the Error equation. (Absolute value).\n",
    "* **Best For:**\n",
    "    * **Feature Selection:** When you have 1000 columns but suspect only 10 are important. Lasso will delete the other 990.\n",
    "* **Analogy:** In a meeting, you kick out the useless people. Only the important speakers remain.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. üèÜ The Grand Decision Table\n",
    "\n",
    "| Model | Complexity | Risk of Overfitting | Math Penalty | Handling \"Trash\" Columns | Best Use Case |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **Simple/Multiple** | Low | Low (Underfitting?) | None | Uses everything | The starting point for any project. |\n",
    "| **Polynomial** | High | **Very High** | None | Uses everything | When data is clearly **curved** (Non-linear). |\n",
    "| **Ridge (L2)** | Medium | Low | **Squared ($Slope^2$)** | Shrinks them (0.01) | When data is noisy or features are correlated. |\n",
    "| **Lasso (L1)** | Medium | Low | **Absolute ($|Slope|$)** | **Deletes them (0.0)** | When you have too many columns (Feature Selection). |\n",
    "\n",
    "---\n",
    "\n",
    "### üêç Python Tip: Which code to write?\n",
    "\n",
    "* **Standard:** `LinearRegression()`\n",
    "* **If Overfitting:** `Ridge(alpha=1.0)`\n",
    "* **If Too Many Columns:** `Lasso(alpha=0.1)`\n",
    "\n",
    "*(Note: `alpha` is the strength of the punishment. Higher alpha = Simpler model).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559fb2f8-2602-47b2-9c92-c785a8baa6f4",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Regularization: Ridge (L2) vs. Lasso (L1)\n",
    "\n",
    "In Machine Learning, our biggest fear is **Overfitting** (Memorizing).\n",
    "When a model overfits, it learns the \"noise\" in the training data and fails on new data.\n",
    "To fix this, we use **Regularization** (Punishment). We punish the model if it tries to use large coefficients (weights).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The \"Strict Teacher\" Analogy üë©‚Äçüè´\n",
    "\n",
    "Imagine your features (columns) are students in a classroom. They are all shouting answers to predict the target.\n",
    "\n",
    "### üîπ Ridge Regression (L2) - \"The Volume Control\"\n",
    "* **Scenario:** The teacher says, *\"No one is allowed to shout. Lower your voice!\"*\n",
    "* **Action:** If a student screams (Weight = 10,000), the teacher forces them to whisper (Weight = 5).\n",
    "* **Result:** Everyone stays in the class, but they are quiet. No single student dominates the decision.\n",
    "* **Technical:** It **shrinks** coefficients towards zero, but they never reach exactly zero.\n",
    "\n",
    "### üî∏ Lasso Regression (L1) - \"The Silencer\"\n",
    "* **Scenario:** The teacher says, *\"If you are whispering useless things, GET OUT.\"*\n",
    "* **Action:** If a student is not helpful enough to pay the \"tax,\" the teacher kicks them out.\n",
    "* **Result:** Only the smart students remain. The room is less crowded.\n",
    "* **Technical:** It sets coefficients to **exactly Zero (0.0)**. It performs **Feature Selection**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Ridge Regression (L2 Regularization)\n",
    "\n",
    "**Formula:** $$\\text{Cost} = \\text{Error} + \\lambda \\sum (\\text{slope})^2$$\n",
    "\n",
    "* **Why Square ($^2$)?** Squaring a large number makes it HUGE ($10^2 = 100$). This creates a massive penalty for large weights, forcing the model to pick small numbers.\n",
    "* **When to use?**\n",
    "    * When you want to keep **ALL** features (e.g., all pixels in an image).\n",
    "    * When you have **Multicollinearity** (many duplicate features).\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "# alpha is the \"Strictness\" of the teacher.\n",
    "# alpha=0: Linear Regression (No rules).\n",
    "# alpha=100: Very strict (Tiny weights).\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da33763e-2989-4b72-835d-cd0b7c4abe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correlation with Price ---\n",
      "Rooms        0.992150\n",
      "Age         -0.126291\n",
      "Shoe_Size    0.014847\n",
      "Price        1.000000\n",
      "Name: Price, dtype: float64\n",
      "\n",
      "--- Lasso Weights ---\n",
      "Rooms: 10021.85\n",
      "Age:   -100.27\n",
      "Shoes: 49.02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# 1. Create Data\n",
    "# Target (Price) depends strongly on 'Rooms', weakly on 'Age', not at all on 'Shoes'\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'Rooms': np.random.randint(1, 6, 100),\n",
    "    'Age': np.random.randint(0, 50, 100),\n",
    "    'Shoe_Size': np.random.randint(35, 45, 100) # Random\n",
    "})\n",
    "# Price formula\n",
    "df['Price'] = (df['Rooms'] * 10000) - (df['Age'] * 100) + np.random.normal(0, 1000, 100)\n",
    "\n",
    "# 2. Check Correlation (How related are they?)\n",
    "print(\"--- Correlation with Price ---\")\n",
    "print(df.corr()['Price'])\n",
    "\n",
    "# 3. Run Lasso\n",
    "X = df[['Rooms', 'Age', 'Shoe_Size']]\n",
    "y = df['Price']\n",
    "\n",
    "lasso = Lasso(alpha=20) # High Tax\n",
    "lasso.fit(X, y)\n",
    "\n",
    "print(\"\\n--- Lasso Weights ---\")\n",
    "print(f\"Rooms: {lasso.coef_[0]:.2f}\")     # High weight\n",
    "print(f\"Age:   {lasso.coef_[1]:.2f}\")     # Small negative weight\n",
    "print(f\"Shoes: {lasso.coef_[2]:.2f}\")     # ZERO (Killed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5b2bf-dc43-4d2f-aa8e-20e6a0b7564d",
   "metadata": {},
   "source": [
    "Feature,  Ridge (L2),                                          Lasso (L1)\n",
    "Does it reach Zero?,  NO (0.0001),                            YES (0.0)\n",
    "Feature Selection? NO                                       Yes (Deletes columns)\n",
    "Best For? Preventing Overfitting while keeping data        Cleaning complex datasets with many useless columns.\n",
    "Math Penalty,Squared (x2)                                  Absolute ($"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
