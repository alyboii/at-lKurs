{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8537fbf-96c0-4d42-9997-aeb60c19bcef",
   "metadata": {},
   "source": [
    "# üöÄ Regression Models Encyclopedia: Which One Should I Choose?\n",
    "\n",
    "In Data Science, there is no \"Single Best Model.\" You must choose the right weapon for the battle. Here is your complete guide.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Basic Models (No Punishment)\n",
    "The goal of these models is simply to minimize the **Error** (Residuals). They do not care if the math gets complex.\n",
    "\n",
    "### üîπ 1. Simple Linear Regression\n",
    "* **The Logic:** \"One Input, One Output.\" Draws a straight line.\n",
    "* **The Formula:** $$y = b_0 + b_1x$$\n",
    "* **Real World Example:** Predicting a child's **Height** based ONLY on their **Age**.\n",
    "* **When to use:** When you have a single feature and the relationship looks straight.\n",
    "\n",
    "### üîπ 2. Multiple Linear Regression\n",
    "* **The Logic:** \"Many Inputs, One Output.\"\n",
    "* **The Formula:** $$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$$\n",
    "* **Critical Rule:** You MUST use **StandardScaler**! Otherwise, big numbers (Salary: 50,000) will dominate small numbers (Age: 30).\n",
    "* **Real World Example:** Predicting **House Price** based on *Size, Location, Floor, and Age*.\n",
    "\n",
    "### üîπ 3. Polynomial Regression\n",
    "* **The Logic:** \"The world is curved, not flat.\" We use powers ($x^2, x^3$) to capture curves.\n",
    "* **The Math:** It is actually Multiple Regression, but with engineered features:\n",
    "  $$y = b_0 + b_1x + b_2x^2 + b_3x^3$$\n",
    "* **The Risk:** If you choose a high Degree (e.g., 10), the model will **Overfit** (memorize the noise).\n",
    "* **Real World Example:** **Temperature Prediction**. It is cold in the morning, hot at noon, and cold at night. A straight line fails here; you need a \"U\" curve.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Regularization Models (The Punishers)\n",
    "If your model is **Overfitting** (memorizing), we add a \"Penalty\" to the math.\n",
    "\n",
    "> **üí° The Philosophy:** \"You can make a model, but you are NOT allowed to use huge coefficients (weights). Keep it simple.\"\n",
    "\n",
    "### üî∏ 4. Ridge Regression (L2 Regularization) - \"The Volume Control\"\n",
    "* **How it works:** It shrinks the coefficients (weights) towards zero, but they never reach exactly zero.\n",
    "* **The Penalty Math:** Adds $$\\lambda \\sum (\\text{slope})^2$$ to the Error equation.\n",
    "    * Because we **square** the slope, large numbers (like 10,000) get a **huge** penalty. The model is forced to pick small numbers.\n",
    "* **Best For:**\n",
    "    * When you want to keep ALL your features.\n",
    "    * When you have **Multicollinearity** (duplicate features like \"Age in Years\" and \"Age in Months\").\n",
    "* **Analogy:** In a loud meeting, you don't kick anyone out, but you turn down everyone's volume.\n",
    "\n",
    "### üî∏ 5. Lasso Regression (L1 Regularization) - \"The Mute Button\"\n",
    "* **How it works:** It shrinks coefficients all the way to **Zero (0)**. It effectively deletes useless features.\n",
    "* **The Penalty Math:** Adds $$\\lambda \\sum |\\text{slope}|$$ to the Error equation. (Absolute value).\n",
    "* **Best For:**\n",
    "    * **Feature Selection:** When you have 1000 columns but suspect only 10 are important. Lasso will delete the other 990.\n",
    "* **Analogy:** In a meeting, you kick out the useless people. Only the important speakers remain.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. üèÜ The Grand Decision Table\n",
    "\n",
    "| Model | Complexity | Risk of Overfitting | Math Penalty | Handling \"Trash\" Columns | Best Use Case |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **Simple/Multiple** | Low | Low (Underfitting?) | None | Uses everything | The starting point for any project. |\n",
    "| **Polynomial** | High | **Very High** | None | Uses everything | When data is clearly **curved** (Non-linear). |\n",
    "| **Ridge (L2)** | Medium | Low | **Squared ($Slope^2$)** | Shrinks them (0.01) | When data is noisy or features are correlated. |\n",
    "| **Lasso (L1)** | Medium | Low | **Absolute ($|Slope|$)** | **Deletes them (0.0)** | When you have too many columns (Feature Selection). |\n",
    "\n",
    "---\n",
    "\n",
    "### üêç Python Tip: Which code to write?\n",
    "\n",
    "* **Standard:** `LinearRegression()`\n",
    "* **If Overfitting:** `Ridge(alpha=1.0)`\n",
    "* **If Too Many Columns:** `Lasso(alpha=0.1)`\n",
    "\n",
    "*(Note: `alpha` is the strength of the punishment. Higher alpha = Simpler model).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd08e86-0600-4fa2-9857-a9e66cf2cad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d09739f-cebc-401e-9534-543cefeb7b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correlation with Price ---\n",
      "Rooms        0.992150\n",
      "Age         -0.126291\n",
      "Shoe_Size    0.014847\n",
      "Price        1.000000\n",
      "Name: Price, dtype: float64\n",
      "\n",
      "--- Lasso Weights ---\n",
      "Rooms: 10021.85\n",
      "Age:   -100.27\n",
      "Shoes: 49.02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# 1. Create Data\n",
    "# Target (Price) depends strongly on 'Rooms', weakly on 'Age', not at all on 'Shoes'\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'Rooms': np.random.randint(1, 6, 100),\n",
    "    'Age': np.random.randint(0, 50, 100),\n",
    "    'Shoe_Size': np.random.randint(35, 45, 100) # Random\n",
    "})\n",
    "# Price formula\n",
    "df['Price'] = (df['Rooms'] * 10000) - (df['Age'] * 100) + np.random.normal(0, 1000, 100)\n",
    "\n",
    "# 2. Check Correlation (How related are they?)\n",
    "print(\"--- Correlation with Price ---\")\n",
    "print(df.corr()['Price'])\n",
    "\n",
    "# 3. Run Lasso\n",
    "X = df[['Rooms', 'Age', 'Shoe_Size']]\n",
    "y = df['Price']\n",
    "\n",
    "lasso = Lasso(alpha=20) # High Tax\n",
    "lasso.fit(X, y)\n",
    "\n",
    "print(\"\\n--- Lasso Weights ---\")\n",
    "print(f\"Rooms: {lasso.coef_[0]:.2f}\")     # High weight\n",
    "print(f\"Age:   {lasso.coef_[1]:.2f}\")     # Small negative weight\n",
    "print(f\"Shoes: {lasso.coef_[2]:.2f}\")     # ZERO (Killed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06e3ad-02ea-4411-97f0-eb9ce409f63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
